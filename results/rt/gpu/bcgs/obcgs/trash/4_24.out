 Data for JOB [40746,1] offset 0

 ========================   JOB MAP   ========================

 Data for node: ivb110	Num slots: 6	Max slots: 0	Num procs: 6
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 0
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 1
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 2
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 3
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 4
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 5

 Data for node: ivb111	Num slots: 6	Max slots: 0	Num procs: 6
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 6
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 7
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 8
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 9
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 10
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 11

 Data for node: ivb112	Num slots: 6	Max slots: 0	Num procs: 6
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 12
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 13
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 14
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 15
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 16
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 17

 Data for node: ivb113	Num slots: 6	Max slots: 0	Num procs: 6
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 18
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 19
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 20
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 21
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 22
 	Process OMPI jobid: [40746,1] App: 0 Process rank: 23

 =============================================================
[ivb110:00338] MCW rank 0 bound to socket 0[core 0[hwt 0]]: [B/././././././././.][./././././././././.]
[ivb110:00338] MCW rank 1 bound to socket 0[core 1[hwt 0]]: [./B/./././././././.][./././././././././.]
[ivb110:00338] MCW rank 2 bound to socket 0[core 2[hwt 0]]: [././B/././././././.][./././././././././.]
[ivb110:00338] MCW rank 3 bound to socket 0[core 3[hwt 0]]: [./././B/./././././.][./././././././././.]
[ivb110:00338] MCW rank 4 bound to socket 0[core 4[hwt 0]]: [././././B/././././.][./././././././././.]
[ivb110:00338] MCW rank 5 bound to socket 0[core 5[hwt 0]]: [./././././B/./././.][./././././././././.]
[ivb112:39958] MCW rank 17 bound to socket 0[core 5[hwt 0]]: [./././././B/./././.][./././././././././.]
[ivb112:39958] MCW rank 12 bound to socket 0[core 0[hwt 0]]: [B/././././././././.][./././././././././.]
[ivb112:39958] MCW rank 13 bound to socket 0[core 1[hwt 0]]: [./B/./././././././.][./././././././././.]
[ivb112:39958] MCW rank 14 bound to socket 0[core 2[hwt 0]]: [././B/././././././.][./././././././././.]
[ivb112:39958] MCW rank 15 bound to socket 0[core 3[hwt 0]]: [./././B/./././././.][./././././././././.]
[ivb112:39958] MCW rank 16 bound to socket 0[core 4[hwt 0]]: [././././B/././././.][./././././././././.]
[ivb111:14251] MCW rank 7 bound to socket 0[core 1[hwt 0]]: [./B/./././././././.][./././././././././.]
[ivb111:14251] MCW rank 8 bound to socket 0[core 2[hwt 0]]: [././B/././././././.][./././././././././.]
[ivb111:14251] MCW rank 9 bound to socket 0[core 3[hwt 0]]: [./././B/./././././.][./././././././././.]
[ivb111:14251] MCW rank 10 bound to socket 0[core 4[hwt 0]]: [././././B/././././.][./././././././././.]
[ivb111:14251] MCW rank 11 bound to socket 0[core 5[hwt 0]]: [./././././B/./././.][./././././././././.]
[ivb111:14251] MCW rank 6 bound to socket 0[core 0[hwt 0]]: [B/././././././././.][./././././././././.]
[ivb113:29650] MCW rank 18 bound to socket 0[core 0[hwt 0]]: [B/././././././././.][./././././././././.]
[ivb113:29650] MCW rank 19 bound to socket 0[core 1[hwt 0]]: [./B/./././././././.][./././././././././.]
[ivb113:29650] MCW rank 20 bound to socket 0[core 2[hwt 0]]: [././B/././././././.][./././././././././.]
[ivb113:29650] MCW rank 21 bound to socket 0[core 3[hwt 0]]: [./././B/./././././.][./././././././././.]
[ivb113:29650] MCW rank 22 bound to socket 0[core 4[hwt 0]]: [././././B/././././.][./././././././././.]
[ivb113:29650] MCW rank 23 bound to socket 0[core 5[hwt 0]]: [./././././B/./././.][./././././././././.]
0
1
0
2
1
3
4
5
2
1
0
3
4
5
2
3
4
5
1
0
2
3
4
5
/*---------------------------------------------------------------------------*\
| =========                 |                                                 |
| \\      /  F ield         | foam-extend: Open Source CFD                    |
|  \\    /   O peration     | Version:     3.1                                |
|   \\  /    A nd           | Web:         http://www.extend-project.de       |
|    \\/     M anipulation  |                                                 |
\*---------------------------------------------------------------------------*/
Build    : 3.1-e2da948002a2
Exec     : interFoam -parallel
Date     : May 17 2016
Time     : 13:11:28
Host     : ivb110
PID      : 403
CtrlDict : /home-2/vplatonov/foam/foam-extend-3.1/etc/controlDict
Case     : /home/vplatonov/tests/rt12mln
nProcs   : 24
Slaves : 
23
(
ivb110.411
ivb110.407
ivb110.406
ivb110.417
ivb110.422
ivb111.14313
ivb111.14310
ivb111.14312
ivb111.14314
ivb111.14311
ivb111.14319
ivb112.39990
ivb112.39997
ivb112.40008
ivb112.40014
ivb112.40020
ivb112.40025
ivb113.29692
ivb113.29694
ivb113.29696
ivb113.29706
ivb113.29712
ivb113.29717
)

Pstream initialized with:
    floatTransfer     : 0
    nProcsSimpleSum   : 0
    commsType         : blocking
SigFpe   : Enabling floating point exception trapping (FOAM_SIGFPE).

// * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * //
Create time

Create mesh for time = 0


Reading g
[11] 
[11] 
[11] --> FOAM FATAL IO ERROR: 
[11] cannot open file
[11] 
[11] file: /home/vplatonov/tests/rt12mln/processor11/0/pd at line 0.
[11] 
[11]     From function regIOobject::readStream()
[11]     in file db/regIOobject/regIOobjectRead.C at line 61.
[11] 
FOAM parallel run exiting
[11] 
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 11 in communicator MPI_COMM_WORLD 
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
Reading field pd

[13] 
[13] 
[13] --> FOAM FATAL IO ERROR: 
[13] cannot open file
[13] 
[13] file: /home/vplatonov/tests/rt12mln/processor13/0/pd at line 0.
[13] 
[13]     From function regIOobject::readStream()
[13]     in file db/regIOobject/regIOobjectRead.C at line 61.
[13] 
FOAM parallel run exiting
[13] 
[20] 
[20] 
[20] --> FOAM FATAL IO ERROR: 
[20] cannot open file
[20] 
[20] file: /home/vplatonov/tests/rt12mln/processor20/0/pd at line 0.
[20] 
[20]     From function regIOobject::readStream()
[20]     in file db/regIOobject/regIOobjectRead.C at line 61.
[20] 
FOAM parallel run exiting
[20] 
[14] 
[14] 
[14] --> FOAM FATAL IO ERROR: 
[14] cannot open file
[14] 
[14] file: /home/vplatonov/tests/rt12mln/processor14/0/pd at line 0.
[14] 
[14]     From function regIOobject::readStream()
[14]     in file db/regIOobject/regIOobjectRead.C at line 61.
[14] 
FOAM parallel run exiting
[14] 
[22] 
[22] 
[22] --> FOAM FATAL IO ERROR: 
[22] cannot open file
[22] 
[22] file: /home/vplatonov/tests/rt12mln/processor22/0/pd at line 0.
[22] 
[22]     From function regIOobject::readStream()
[22]     in file db/regIOobject/regIOobjectRead.C at line 61.
[22] 
FOAM parallel run exiting
[22] 
Reading field alpha1

[12] 
[12] 
[12] --> FOAM FATAL IO ERROR: 
[12] cannot open file
[12] 
[12] file: /home/vplatonov/tests/rt12mln/processor12/0/pd at line 0.
[12] 
[12]     From function regIOobject::readStream()
[12]     in file db/regIOobject/regIOobjectRead.C at line 61.
[12] 
FOAM parallel run exiting
[12] 
[19] 
[19] 
[19] --> FOAM FATAL IO ERROR: 
[19] cannot open file
[19] 
[19] file: /home/vplatonov/tests/rt12mln/processor19/0/pd at line 0.
[19] 
[19]     From function regIOobject::readStream()
[19]     in file db/regIOobject/regIOobjectRead.C at line 61.
[19] 
FOAM parallel run exiting
[19] 
[15] 
[15] 
[15] --> FOAM FATAL IO ERROR: 
[15] cannot open file
[15] 
[15] file: /home/vplatonov/tests/rt12mln/processor15/0/pd at line 0.
[15] 
[15]     From function regIOobject::readStream()
[15]     in file db/regIOobject/regIOobjectRead.C at line 61.
[15] 
FOAM parallel run exiting
[15] 
[21] 
[21] 
[21] --> FOAM FATAL IO ERROR: 
[21] cannot open file
[21] 
[21] file: /home/vplatonov/tests/rt12mln/processor21/0/pd at line 0.
[21] 
[21]     From function regIOobject::readStream()
[21]     in file db/regIOobject/regIOobjectRead.C at line 61.
[21] 
FOAM parallel run exiting
[21] 
[16] 
[16] 
[16] --> FOAM FATAL IO ERROR: 
[16] cannot open file
[16] 
[16] file: /home/vplatonov/tests/rt12mln/processor16/0/pd at line 0.
[16] 
[16]     From function regIOobject::readStream()
[16]     in file db/regIOobject/regIOobjectRead.C at line 61.
[16] 
FOAM parallel run exiting
[16] 
[18] 
[18] 
[18] --> FOAM FATAL IO ERROR: 
[18] cannot open file
[18] 
[18] file: /home/vplatonov/tests/rt12mln/processor18/0/pd at line 0.
[18] 
[18]     From function regIOobject::readStream()
[18]     in file db/regIOobject/regIOobjectRead.C at line 61.
[18] 
FOAM parallel run exiting
[18] 
[17] 
[17] 
[17] --> FOAM FATAL IO ERROR: 
[17] cannot open file
[17] 
[17] file: /home/vplatonov/tests/rt12mln/processor17/0/pd at line 0.
[17] 
[17]     From function regIOobject::readStream()
[17]     in file db/regIOobject/regIOobjectRead.C at line 61.
[17] 
FOAM parallel run exiting
[17] 
[23] 
[23] 
[23] --> FOAM FATAL IO ERROR: 
[23] cannot open file
[23] 
[23] file: /home/vplatonov/tests/rt12mln/processor23/0/pd at line 0.
[23] 
[23]     From function regIOobject::readStream()
[23]     in file db/regIOobject/regIOobjectRead.C at line 61.
[23] 
FOAM parallel run exiting
[23] 
Reading field U

Reading/calculating face flux field phi

Reading transportProperties

Selecting incompressible transport model Newtonian
Selecting incompressible transport model Newtonian
Calculating field g.h

[ivb110:00338] 12 more processes have sent help message help-mpi-api.txt / mpi-abort
[ivb110:00338] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
